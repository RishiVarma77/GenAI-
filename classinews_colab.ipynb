{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "507e6076",
   "metadata": {},
   "source": [
    "# ClassiNews — Colab Notebook\n",
    "\n",
    "**Intelligent News Article Categorization System**\n",
    "\n",
    "This notebook contains a full pipeline for the ClassiNews project:\n",
    "- dataset loading (use your uploaded ZIP or fallback to AG News),\n",
    "- preprocessing, EDA, baseline TF-IDF + Logistic Regression,\n",
    "- Transformer fine-tuning (DistilBERT) using Hugging Face, and\n",
    "- evaluation + inference examples.\n",
    "\n",
    "**How to use:** Open this notebook in Google Colab. If you have a project ZIP (like `(genAi)_final project.zip`), upload it to Colab or mount Google Drive and place the zip in the working directory. The notebook will try to detect and use your uploaded dataset; otherwise it will use the AG News dataset as a fallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Setup — run this first\n",
    "# Install required libraries in Colab\n",
    "!pip install -q transformers datasets evaluate scikit-learn matplotlib seaborn sentencepiece\n",
    "!pip install -q accelerate\n",
    "# For progress bars\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "print(\"Setup complete. Libraries installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b15a2c",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "You have two options:\n",
    "1. **Upload your project ZIP** (e.g. `(genAi)_final project.zip`) with CSV(s) containing news text and labels. Place it in the notebook working directory or mount Drive. The notebook will look for CSV files with columns named `text` and `label` (or `category`).\n",
    "\n",
    "2. **Fallback**: If no CSV is found, the notebook automatically loads the **AG News** dataset (4 classes) from the `datasets` library for demonstration.\n",
    "\n",
    "**If you use your own dataset**, ensure it has at least two columns: `text` (article content/title) and `label` (integer index or string category)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0da0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to detect uploaded CSV files or unzip a provided zip file.\n",
    "import os, glob, zipfile, pandas as pd\n",
    "DATA_PATH = '/content'  # Colab working directory\n",
    "found_csv = glob.glob(os.path.join(DATA_PATH, '*.csv'))\n",
    "\n",
    "# If a zip file is present (like your uploaded project zip), unzip it and search again\n",
    "zips = glob.glob(os.path.join(DATA_PATH, '*.zip'))\n",
    "if zips:\n",
    "    for z in zips:\n",
    "        try:\n",
    "            zipfile.ZipFile(z).extractall(DATA_PATH)\n",
    "            print(f'Extracted: {z}')\n",
    "        except Exception as e:\n",
    "            print('Could not extract', z, e)\n",
    "    found_csv = glob.glob(os.path.join(DATA_PATH, '**', '*.csv'), recursive=True)\n",
    "\n",
    "found_csv[:10], len(found_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV if found (prefers first matching CSV). Otherwise load AG News.\n",
    "from datasets import load_dataset, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "if found_csv:\n",
    "    csv_path = found_csv[0]\n",
    "    print('Using CSV:', csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Try to standardize columns\n",
    "    if 'text' not in df.columns:\n",
    "        # try common alternatives\n",
    "        for alt in ['content','article','body','headline']:\n",
    "            if alt in df.columns:\n",
    "                df = df.rename(columns={alt:'text'})\n",
    "                break\n",
    "    if 'label' not in df.columns and 'category' in df.columns:\n",
    "        df = df.rename(columns={'category':'label'})\n",
    "    display(df.head())\n",
    "    # If labels are strings, convert to categorical codes later.\n",
    "    dataset = Dataset.from_pandas(df[['text','label']])\n",
    "else:\n",
    "    print('No CSV found — loading AG News as fallback.')\n",
    "    dataset = load_dataset('ag_news')  # has train/test splits\n",
    "    display(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7ce85",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We'll show a simple preprocessing pipeline: clean text, lowercase, remove extra spaces. For transformer models we won't remove stopwords or stem (transformers handle raw text better). For TF-IDF baseline we'll apply simple tokenization as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112c4b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic preprocessing utilities\n",
    "import re\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''\n",
    "    text = text.replace('\\n',' ').replace('\\r',' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)   # collapse whitespace\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply to dataset (datasets.Dataset or HuggingFace dataset)\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "if hasattr(dataset, 'column_names'):\n",
    "    # single split (from CSV)\n",
    "    dataset = dataset.map(lambda x: {'text': clean_text(x['text'])})\n",
    "    # if labels are strings convert them to ids\n",
    "    sample_label = dataset[0].get('label', None)\n",
    "    if isinstance(sample_label, str):\n",
    "        labels = sorted(list(set(dataset['label'])))\n",
    "        label2id = {l:i for i,l in enumerate(labels)}\n",
    "        dataset = dataset.map(lambda x: {'label': label2id[x['label']]}, remove_columns=dataset.column_names)\n",
    "else:\n",
    "    # dataset like AG News with train/test splits\n",
    "    dataset = dataset.map(lambda x: {'text': clean_text(x['text'])})\n",
    "print('Preprocessing finished. Sample:')\n",
    "print(dataset['train'][0] if 'train' in dataset.keys() else dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ad7d5",
   "metadata": {},
   "source": [
    "## Train / Validation / Test split\n",
    "If dataset came with train/test splits, we'll use them. Otherwise create a split (80/10/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "# If dataset has train/test already (like AG News), keep splits. Otherwise split.\n",
    "if 'train' in dataset.keys() and 'test' in dataset.keys():\n",
    "    ds = DatasetDict({\n",
    "        'train': dataset['train'],\n",
    "        'test': dataset['test']\n",
    "    })\n",
    "    # Create validation from train\n",
    "    ds['train'], ds['validation'] = ds['train'].train_test_split(test_size=0.1, seed=42).values()\n",
    "else:\n",
    "    # single split dataset\n",
    "    tmp = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    ds = DatasetDict({\n",
    "        'train': tmp['train'],\n",
    "        'test': tmp['test']\n",
    "    })\n",
    "    ds['train'], ds['validation'] = ds['train'].train_test_split(test_size=0.111, seed=42).values()  # ~10% val of original\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546dbec",
   "metadata": {},
   "source": [
    "## Baseline: TF-IDF + Logistic Regression\n",
    "We'll vectorize text using TF-IDF and train a scikit-learn logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7149922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datasets to pandas for scikit-learn\n",
    "import pandas as pd, numpy as np\n",
    "train_df = pd.DataFrame(ds['train'])\n",
    "val_df = pd.DataFrame(ds['validation'])\n",
    "test_df = pd.DataFrame(ds['test'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "pipe = Pipeline([('tfidf', tfidf), ('clf', clf)])\n",
    "\n",
    "print('Training TF-IDF + LogisticRegression baseline...')\n",
    "pipe.fit(train_df['text'].astype(str), train_df['label'])\n",
    "\n",
    "preds = pipe.predict(test_df['text'].astype(str))\n",
    "print('Accuracy:', accuracy_score(test_df['label'], preds))\n",
    "print('\\nClassification Report:\\n', classification_report(test_df['label'], preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1005f9f2",
   "metadata": {},
   "source": [
    "## Transformer fine-tuning (DistilBERT)\n",
    "We'll fine-tune a DistilBERT base model (faster & smaller). This uses Hugging Face Transformers `Trainer` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59036a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for fine-tuning\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized = ds.map(tokenize_fn, batched=True, remove_columns=[c for c in ds['train'].column_names if c!='label' and c!='text'])\n",
    "tokenized = tokenized.rename_column('label','labels')\n",
    "tokenized.set_format('torch')\n",
    "\n",
    "num_labels = len(set(train_df['label'])) if 'label' in train_df.columns else (max(train_df['label'])+1)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45abd28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute metrics\n",
    "import evaluate\n",
    "metric = evaluate.load('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "# Training arguments - keep small for Colab / demo\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized['train'],\n",
    "    eval_dataset=tokenized['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training (may take long depending on runtime)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7675b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_tok = tokenized['test']\n",
    "preds_output = trainer.predict(test_tok)\n",
    "preds = np.argmax(preds_output.predictions, axis=-1)\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print('Test accuracy:', accuracy_score(test_tok['labels'].numpy(), preds))\n",
    "print(classification_report(test_tok['labels'].numpy(), preds, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664e51a",
   "metadata": {},
   "source": [
    "## Save model & inference example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d9524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model locally (you can then download or push to Drive)\n",
    "model.save_pretrained('./classinews_distilbert')\n",
    "tokenizer.save_pretrained('./classinews_distilbert')\n",
    "print('Saved model to ./classinews_distilbert')\n",
    "\n",
    "\n",
    "# Inference example function\n",
    "from transformers import pipeline\n",
    "classifier = pipeline('text-classification', model='./classinews_distilbert', tokenizer='./classinews_distilbert', return_all_scores=False)\n",
    "\n",
    "def predict(text):\n",
    "    text = clean_text(text)\n",
    "    out = classifier(text, truncation=True, max_length=256)\n",
    "    return out\n",
    "\n",
    "# Demo\n",
    "sample_text = \"President signs new economic bill to boost small businesses and create jobs.\"\n",
    "print('Sample prediction:', predict(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d2f074",
   "metadata": {},
   "source": [
    "## Notes & next steps\n",
    "- For multi-label tasks, convert training & loss to support multi-label (BCEWithLogitsLoss) and prepare multi-hot labels.\n",
    "- For larger datasets or better performance, use a GPU runtime (Colab GPU/TPU), increase epochs, and tune learning rate.\n",
    "- Add data augmentation, class balancing, or use ensemble methods for stronger baselines.\n",
    "\n",
    "---\n",
    "\n",
    "**End of notebook**"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
